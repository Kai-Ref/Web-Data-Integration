{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teams(league_url, years, league_id):\n",
    "    '''Scrape team information for certain leagues and given years.'''\n",
    "    df = pd.DataFrame()\n",
    "    if type(years) == int:\n",
    "        years = [years]\n",
    "    for year in years:\n",
    "        url = league_url + f\"/plus/?saison_id={year}\"\n",
    "\n",
    "        # exception handling\n",
    "        r = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html = urlopen(r)\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Get the team names\n",
    "        team_rows = bs.find('table', {'class': 'items'}).find_all('td', {\"class\":\"hauptlink no-border-links\"})\n",
    "        \n",
    "        teams = {}\n",
    "        for row in team_rows:\n",
    "            team_name = row.text.strip().split(' \\\\')[0]\n",
    "            team_href = row.find('a')['href']\n",
    "            team_id = team_href.split('/')[4]\n",
    "            teams[team_name]={'href': team_href, 'id': team_id}\n",
    "        # TODO maybe also add the market value of the team\n",
    "        # turn into df\n",
    "        teams_df = pd.DataFrame.from_dict(teams, orient=\"index\").reset_index(drop=False, names=\"team_name\")\n",
    "        teams_df[\"year\"] = year\n",
    "        teams_df[\"league_id\"] = league_id\n",
    "        teams_df[\"top_flight\"] = 1\n",
    "        df = pd.concat([df, teams_df], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_players(team_url):\n",
    "    r = Request(team_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urlopen(r)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    # Get the team names\n",
    "    try:\n",
    "        # Find all 'a' tags with 'href' attributes\n",
    "        player_rows = bs.find('table', {'class': 'items'}).find_all('a', href=True)\n",
    "        # Define the regex pattern to match player URLs\n",
    "        regex_pattern = r\"/[^/]+/profil/spieler/\\d+\"\n",
    "        players = {}\n",
    "        for row in player_rows:\n",
    "            if re.search(regex_pattern, row['href']):\n",
    "                player_name = row.text\n",
    "                player_href = row[\"href\"]\n",
    "                player_id = player_href.split(\"/\")[-1]\n",
    "                players[player_id] = {\"player_href\": player_href, \"player_name\":player_name}\n",
    "\n",
    "        player_dates, player_numbers = [], []\n",
    "        team_rows = bs.find('table', {'class': 'items'}).find_all('td', {\"class\":\"zentriert\"})\n",
    "        for row in team_rows:\n",
    "            if row.get_text()== '':\n",
    "                continue\n",
    "            elif len(row.get_text())>=3:\n",
    "                player_dates.append(row.get_text())\n",
    "            else:\n",
    "                player_numbers.append(row.get_text())\n",
    "\n",
    "\n",
    "        # not ideal but add the dates and numbers based on their index position\n",
    "        if (len(players.keys()) == len(player_dates)) & (len(players.keys()) == len(player_numbers)):\n",
    "            for player_id in players.keys():\n",
    "                players[player_id][\"Birthday\"] = player_dates[list(players.keys()).index(player_id)]\n",
    "                players[player_id][\"Number\"] = player_numbers[list(players.keys()).index(player_id)]\n",
    "        elif(len(players.keys()) == len(player_dates)):\n",
    "            for player_id in players.keys():\n",
    "                players[player_id][\"Birthday\"] = player_dates[list(players.keys()).index(player_id)]\n",
    "            print(f\"Not matching numbers {team_url}\")\n",
    "        elif(len(players.keys()) == len(player_numbers)):\n",
    "            for player_id in players.keys():\n",
    "                players[player_id][\"Number\"] = player_numbers[list(players.keys()).index(player_id)]\n",
    "            print(f\"Not matching dates {team_url}\")\n",
    "        else:\n",
    "            print(f\"Not matching dates and numbers {team_url}\")\n",
    "        # Create a DataFrame from the dictionary\n",
    "        player_df = pd.DataFrame.from_dict(players, orient='index').reset_index(drop=False, names=\"player_id\")\n",
    "    except AttributeError:\n",
    "        print(f\"No data for  {team_url}\")\n",
    "        player_df = pd.DataFrame()\n",
    "    return player_df\n",
    "\n",
    "def get_player_info(url):\n",
    "    # retrieves information from a players web site\n",
    "    r = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urlopen(r)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    player_info = {}\n",
    "    player_info[\"player_id\"] = url.split(\"/\")[-1]\n",
    "\n",
    "    # Get the team names\n",
    "    hrefs, transfer_years, club_ids = [], [], []\n",
    "    grid = bs.find_all(\"div\", {\"class\":\"tm-player-transfer-history-grid\"})\n",
    "    for entry in grid:\n",
    "        old_club = entry.find(\"div\", {\"class\":\"tm-player-transfer-history-grid__old-club\"})\n",
    "        if None == old_club: # handle None matches\n",
    "            continue\n",
    "        else:\n",
    "            if \"grid__heading\" in old_club[\"class\"]:# exclude the header\n",
    "                continue\n",
    "            try:\n",
    "                href = old_club.find(\"a\", {\"class\":\"tm-player-transfer-history-grid__club-link\"})[\"href\"]\n",
    "            except TypeError as e:\n",
    "                href = old_club.find(\"a\")[\"href\"]\n",
    "            transfer_year = href.split(\"/\")[-1]\n",
    "            club_id = href.split(\"/\")[-3]\n",
    "            hrefs.append(href)\n",
    "            transfer_years.append(transfer_year)\n",
    "            club_ids.append(club_id)\n",
    "    player_info[\"transfer_hrefs\"] = hrefs\n",
    "    player_info[\"transfer_years\"] = transfer_years\n",
    "\n",
    "    # current club id\n",
    "    current_club_id = []\n",
    "    for entry in grid:\n",
    "        old_club = entry.find(\"div\", {\"class\":\"tm-player-transfer-history-grid__new-club\"})\n",
    "        if None == old_club: # handle None matches\n",
    "            continue\n",
    "        else:\n",
    "            if \"grid__heading\" in old_club[\"class\"]:# exclude the header\n",
    "                continue\n",
    "            href = old_club.find(\"a\")[\"href\"]\n",
    "            club_id = href.split(\"/\")[-3]\n",
    "            current_club_id.append(club_id)\n",
    "            break # stop after the first found element\n",
    "    player_info[\"current_club\"] = current_club_id\n",
    "    player_info[\"transfer_club_ids\"] = club_ids\n",
    "\n",
    "    # market value\n",
    "    current_mv = bs.find_all(\"div\", {\"class\":\"tm-player-market-value-development__current-value\"})\n",
    "    for entry in current_mv:\n",
    "        player_current_mv = entry.get_text()\n",
    "        player_info[\"current_mv\"] = player_current_mv\n",
    "\n",
    "    # max market value\n",
    "    max_mv = bs.find_all(\"div\", {\"class\":\"tm-player-market-value-development__max-value\"})\n",
    "    for entry in max_mv:\n",
    "        player_max_mv = entry.get_text()\n",
    "        player_info[\"max_mv\"]= player_max_mv\n",
    "    \n",
    "    # position\n",
    "    player_positions = []\n",
    "    positions = bs.find_all(\"dd\", {\"class\":\"detail-position__position\"})\n",
    "    for position in positions:\n",
    "        player_positions.append(position.get_text())\n",
    "    if len(player_positions)>0:\n",
    "        player_main_position = [player_positions[0]]\n",
    "        player_info[\"main_position\"] = player_main_position\n",
    "    if len(player_positions)>1: \n",
    "        player_other_positions = player_positions[1:]\n",
    "        player_info[\"other_positions\"] = player_other_positions\n",
    "\n",
    "    # nationality\n",
    "    player = bs.find('div', {'class': 'info-table'})\n",
    "    nations = player.find_all(\"img\", {\"class\": \"flaggenrahmen\"})\n",
    "    player_nations = []\n",
    "    for nation in nations:\n",
    "        if \"lazy\" in nation[\"class\"]:\n",
    "            continue\n",
    "        player_nations.append(nation[\"title\"])\n",
    "    player_info[\"nationality\"] = player_nations\n",
    "\n",
    "    player_info = {key: [value] for key, value in player_info.items()}\n",
    "    player_info_df = pd.DataFrame(player_info)\n",
    "    return player_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def load_players_info_for_team(team_url, base_url):\n",
    "    # combine the information derived from get_players and get_player_info\n",
    "    team_id = team_url.split('/')[6]\n",
    "    df = get_players(team_url)\n",
    "    # stop here and then continue with additional info\n",
    "    additional_df = pd.DataFrame()\n",
    "    for href in df[\"player_href\"]:#tqdm(df['player_href'], total=len(df)):#df.player_href:\n",
    "        # print(f\"Starting {list(df.player_href).index(href)+1}/{len(list(df.player_href))} - {datetime.datetime.now()}\")\n",
    "        href = base_url + href\n",
    "        player_info_df = get_player_info(href)\n",
    "        additional_df = pd.concat([additional_df, player_info_df], axis=0)\n",
    "    df = pd.merge(df, additional_df, on=\"player_id\", how=\"left\")\n",
    "    return df\n",
    "\n",
    "def get_players_for_all_teams(df):\n",
    "    # get the players for all teams in the league\n",
    "    players_df = pd.DataFrame()\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # maybe dont include the additional info for players but only scrape the team site\n",
    "        player_df=load_players_info_for_team(\"https://www.transfermarkt.com\" + row.href, \"https://www.transfermarkt.com\")\n",
    "        # player_df = get_players(\"https://www.transfermarkt.com\" + row.href)\n",
    "        players_df = pd.concat([players_df, player_df], axis=0)\n",
    "    if \"Unnamed: 0\" in players_df.columns:\n",
    "        players_df = players_df.drop(\"Unnamed: 0\", axis=1)\n",
    "    '''\n",
    "    columns_with_list_type = [\"transfer_years\", \"transfer_hrefs\", \"transfer_club_ids\", \"main_position\", \"other_positions\", \"nationality\"]\n",
    "    columns_with_list_type = [column for column in columns_with_list_type if column in players_df.columns]\n",
    "    for column in columns_with_list_type:\n",
    "        players_df[column] = players_df[column].apply(lambda x: str(x))\n",
    "    subset = [\"players\", \"player_href\", \"player_id\"]\n",
    "    '''\n",
    "    players_df = players_df.drop_duplicates(subset = \"player_id\").reset_index(drop=True)\n",
    "    return players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1/8 - 2023-10-03 21:35:37.795657\n",
      "Finished 1/8 - 2023-10-03 21:35:38.733653\n",
      "Starting 2/8 - 2023-10-03 21:35:38.733653\n",
      "Finished 2/8 - 2023-10-03 21:35:40.079178\n",
      "Starting 3/8 - 2023-10-03 21:35:40.079178\n",
      "Finished 3/8 - 2023-10-03 21:35:40.770520\n",
      "Starting 4/8 - 2023-10-03 21:35:40.770520\n",
      "Finished 4/8 - 2023-10-03 21:35:41.562399\n",
      "Starting 5/8 - 2023-10-03 21:35:41.562399\n",
      "Finished 5/8 - 2023-10-03 21:35:42.447632\n",
      "Starting 6/8 - 2023-10-03 21:35:42.447632\n",
      "Finished 6/8 - 2023-10-03 21:35:43.271527\n",
      "Starting 7/8 - 2023-10-03 21:35:43.271527\n",
      "Finished 7/8 - 2023-10-03 21:35:45.156582\n",
      "Starting 8/8 - 2023-10-03 21:35:45.156582\n",
      "Finished 8/8 - 2023-10-03 21:36:20.575016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 2/155 [04:08<5:16:39, 124.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\555ka\\MMDS\\Automatic_Downloader\\Web Data Integration\\Web-Data-Integration\\Webscraping\\scrape_tm.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# print(combined_df[\"href\"])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# access the players for given teams\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m df \u001b[39m=\u001b[39m combined_df\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m players_df \u001b[39m=\u001b[39m get_players_for_all_teams(df\u001b[39m.\u001b[39;49miloc[:,:])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# print(players_df.head())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m players_df[\u001b[39m\"\u001b[39m\u001b[39mcurrent_mv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m players_df[\u001b[39m\"\u001b[39m\u001b[39mcurrent_mv\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mstr\u001b[39m(x)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;32mc:\\Users\\555ka\\MMDS\\Automatic_Downloader\\Web Data Integration\\Web-Data-Integration\\Webscraping\\scrape_tm.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m players_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m tqdm(df\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# maybe dont include the additional info for players but only scrape the team site\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     player_df\u001b[39m=\u001b[39mload_players_info_for_team(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://www.transfermarkt.com\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m row\u001b[39m.\u001b[39;49mhref, \u001b[39m\"\u001b[39;49m\u001b[39mhttps://www.transfermarkt.com\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# player_df = get_players(\"https://www.transfermarkt.com\" + row.href)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     players_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([players_df, player_df], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\555ka\\MMDS\\Automatic_Downloader\\Web Data Integration\\Web-Data-Integration\\Webscraping\\scrape_tm.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m href \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mplayer_href\u001b[39m\u001b[39m\"\u001b[39m]:\u001b[39m#tqdm(df['player_href'], total=len(df)):#df.player_href:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# print(f\"Starting {list(df.player_href).index(href)+1}/{len(list(df.player_href))} - {datetime.datetime.now()}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     href \u001b[39m=\u001b[39m base_url \u001b[39m+\u001b[39m href\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     player_info_df \u001b[39m=\u001b[39m get_player_info(href)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     additional_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([additional_df, player_info_df], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(df, additional_df, on\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplayer_id\u001b[39m\u001b[39m\"\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\555ka\\MMDS\\Automatic_Downloader\\Web Data Integration\\Web-Data-Integration\\Webscraping\\scrape_tm.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m r \u001b[39m=\u001b[39m Request(url, headers\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mMozilla/5.0\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m html \u001b[39m=\u001b[39m urlopen(r)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m bs \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m'\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m player_info \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/555ka/MMDS/Automatic_Downloader/Web%20Data%20Integration/Web-Data-Integration/Webscraping/scrape_tm.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m player_info[\u001b[39m\"\u001b[39m\u001b[39mplayer_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m url\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\555ka\\MMDS\\Automatic_Downloader\\vne\\lib\\site-packages\\bs4\\__init__.py:314\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only \u001b[39m=\u001b[39m parse_only\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(markup, \u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m):        \u001b[39m# It's a file-type object.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     markup \u001b[39m=\u001b[39m markup\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m    315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(markup) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m256\u001b[39m \u001b[39mand\u001b[39;00m (\n\u001b[0;32m    316\u001b[0m         (\u001b[39misinstance\u001b[39m(markup, \u001b[39mbytes\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m markup)\n\u001b[0;32m    317\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(markup, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m markup)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[39m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_markup_is_url(markup):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:460\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_chunked(amt)\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m         \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:583\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    582\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 583\u001b[0m         chunk_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_chunk_left()\n\u001b[0;32m    584\u001b[0m         \u001b[39mif\u001b[39;00m chunk_left \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    585\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:566\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_safe_read(\u001b[39m2\u001b[39m)  \u001b[39m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 566\u001b[0m     chunk_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_next_chunk_size()\n\u001b[0;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:526\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_next_chunk_size\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    525\u001b[0m     \u001b[39m# Read the next chunk size from the file\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    528\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mchunk size\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Acess the teams for given leagues and years\n",
    "# leagues = [\"Premier League\", 'Jupiler Pro League', \"Bundesliga\",'3. Liga', \"Serie A\", \"La Liga\", \"Ligue 1\", \"Eredivisie\",\n",
    "#             'Championship', 'La Liga 2', 'Serie B', '2. Bundesliga', 'Ligue 2', 'Liga Portugal', 'Super Lig', 'Major League Soccer',\n",
    "#             \"Pro League\", \"Super League\"]\n",
    "\n",
    "# league_urls = {\"/bundesliga/startseite/wettbewerb/L1\":1, \"/premier-league/startseite/wettbewerb/GB1\":2,\n",
    "#                 \"/primera-division/startseite/wettbewerb/ES1\":3, \"/serie-a/startseite/wettbewerb/IT1\":4,\n",
    "#                 \"/ligue-1/startseite/wettbewerb/FR1\":5 }\n",
    "\n",
    "\n",
    "# /liga-portugal/startseite/wettbewerb/PO1\n",
    "# /super-lig/startseite/wettbewerb/TR1\n",
    "# /jupiler-pro-league/startseite/wettbewerb/BE1\n",
    "# /eredivisie/startseite/wettbewerb/NL1\n",
    "\n",
    "# league_urls = {\"/bundesliga/startseite/wettbewerb/L2\":6, \"/premier-league/startseite/wettbewerb/GB2\":7,\n",
    "#                \"/primera-division/startseite/wettbewerb/ES2\":8, \"/serie-a/startseite/wettbewerb/IT2\":9,\n",
    "#                \"/ligue-1/startseite/wettbewerb/FR2\":10}\n",
    "\n",
    "league_urls = {\"/liga-portugal/startseite/wettbewerb/PO1\": 11, \"/super-lig/startseite/wettbewerb/TR1\": 12,\n",
    "               \"/jupiler-pro-league/startseite/wettbewerb/BE1\": 13, \"/eredivisie/startseite/wettbewerb/NL1\": 14,\n",
    "               \"/major-league-soccer/startseite/wettbewerb/MLS1\":15, \"/3-liga/startseite/wettbewerb/L3\": 16,\n",
    "               \"/saudi-professional-league/startseite/wettbewerb/SA1\": 17, \"/chinese-super-league/startseite/wettbewerb/CSL\": 18}\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "for league_url in league_urls.keys():\n",
    "    print(f\"Starting {list(league_urls.keys()).index(league_url)+1}/{len(list(league_urls.keys()))} - {datetime.datetime.now()}\")\n",
    "    df = get_teams(\"https://www.transfermarkt.com\" + league_url, [2023], league_urls[league_url])\n",
    "    combined_df = pd.concat([df, combined_df], axis=0)\n",
    "    print(f\"Finished {list(league_urls.keys()).index(league_url)+1}/{len(list(league_urls.keys()))} - {datetime.datetime.now()}\")\n",
    "\n",
    "# print(combined_df[\"href\"])\n",
    "\n",
    "\n",
    "# access the players for given teams\n",
    "df = combined_df.copy()\n",
    "players_df = get_players_for_all_teams(df.iloc[:,:])\n",
    "# print(players_df.head())\n",
    "players_df[\"current_mv\"] = players_df[\"current_mv\"].apply(lambda x: str(x).replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "players_df[\"max_mv\"] = players_df[\"max_mv\"].apply(lambda x: str(x).replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "players_df[\"Birthday\"] = pd.to_datetime(players_df['Birthday'].str.extract('(\\w+ \\d{1,2}, \\d{4})')[0])\n",
    "players_df.to_csv(\"players_df_others.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
