{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teams(league_url, years, league_id):\n",
    "    '''Scrape team information for certain leagues and given years.'''\n",
    "    df = pd.DataFrame()\n",
    "    if type(years) == int:\n",
    "        years = [years]\n",
    "    for year in years:\n",
    "        url = league_url + f\"/plus/?saison_id={year}\"\n",
    "\n",
    "        # exception handling\n",
    "        r = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html = urlopen(r)\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Get the team names\n",
    "        team_rows = bs.find('table', {'class': 'items'}).find_all('td', {\"class\":\"hauptlink no-border-links\"})\n",
    "        \n",
    "        teams = {}\n",
    "        for row in team_rows:\n",
    "            team_name = row.text.strip().split(' \\\\')[0]\n",
    "            team_href = row.find('a')['href']\n",
    "            team_id = team_href.split('/')[4]\n",
    "            teams[team_name]={'href': team_href, 'id': team_id}\n",
    "        # TODO maybe also add the market value of the team\n",
    "        # turn into df\n",
    "        teams_df = pd.DataFrame.from_dict(teams, orient=\"index\").reset_index(drop=False, names=\"team_name\")\n",
    "        teams_df[\"year\"] = year\n",
    "        teams_df[\"league_id\"] = league_id\n",
    "        teams_df[\"top_flight\"] = 1\n",
    "        df = pd.concat([df, teams_df], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_players(team_url):\n",
    "    r = Request(team_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urlopen(r)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    # Get the team names\n",
    "    try:\n",
    "        # Find all 'a' tags with 'href' attributes\n",
    "        player_rows = bs.find('table', {'class': 'items'}).find_all('a', href=True)\n",
    "        # Define the regex pattern to match player URLs\n",
    "        regex_pattern = r\"/[^/]+/profil/spieler/\\d+\"\n",
    "        players = {}\n",
    "        for row in player_rows:\n",
    "            if re.search(regex_pattern, row['href']):\n",
    "                player_name = row.text\n",
    "                player_href = row[\"href\"]\n",
    "                player_id = player_href.split(\"/\")[-1]\n",
    "                players[player_id] = {\"player_href\": player_href, \"player_name\":player_name}\n",
    "\n",
    "        player_dates, player_numbers = [], []\n",
    "        team_rows = bs.find('table', {'class': 'items'}).find_all('td', {\"class\":\"zentriert\"})\n",
    "        for row in team_rows:\n",
    "            if row.get_text()== '':\n",
    "                continue\n",
    "            elif len(row.get_text())>=3:\n",
    "                player_dates.append(row.get_text())\n",
    "            else:\n",
    "                player_numbers.append(row.get_text())\n",
    "\n",
    "\n",
    "        # not ideal but add the dates and numbers based on their index position\n",
    "        if (len(players.keys()) == len(player_dates)) & (len(players.keys()) == len(player_numbers)):\n",
    "            for player_id in players.keys():\n",
    "                players[player_id][\"Birthday\"] = player_dates[list(players.keys()).index(player_id)]\n",
    "                players[player_id][\"Number\"] = player_numbers[list(players.keys()).index(player_id)]\n",
    "        elif(len(players.keys()) == len(player_dates)):\n",
    "            for player_id in players.keys():\n",
    "                players[player_id][\"Birthday\"] = player_dates[list(players.keys()).index(player_id)]\n",
    "            print(f\"Not matching numbers {team_url}\")\n",
    "        elif(len(players.keys()) == len(player_numbers)):\n",
    "            for player_id in players.keys():\n",
    "                players[player_id][\"Number\"] = player_numbers[list(players.keys()).index(player_id)]\n",
    "            print(f\"Not matching dates {team_url}\")\n",
    "        else:\n",
    "            print(f\"Not matching dates and numbers {team_url}\")\n",
    "        # Create a DataFrame from the dictionary\n",
    "        player_df = pd.DataFrame.from_dict(players, orient='index').reset_index(drop=False, names=\"player_id\")\n",
    "    except AttributeError:\n",
    "        print(f\"No data for  {team_url}\")\n",
    "        player_df = pd.DataFrame()\n",
    "    return player_df\n",
    "\n",
    "def get_player_info(url):\n",
    "    # retrieves information from a players web site\n",
    "    r = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urlopen(r)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    player_info = {}\n",
    "    player_info[\"player_id\"] = url.split(\"/\")[-1]\n",
    "\n",
    "    # Get the team names\n",
    "    hrefs, transfer_years, club_ids = [], [], []\n",
    "    grid = bs.find_all(\"div\", {\"class\":\"tm-player-transfer-history-grid\"})\n",
    "    for entry in grid:\n",
    "        old_club = entry.find(\"div\", {\"class\":\"tm-player-transfer-history-grid__old-club\"})\n",
    "        if None == old_club: # handle None matches\n",
    "            continue\n",
    "        else:\n",
    "            if \"grid__heading\" in old_club[\"class\"]:# exclude the header\n",
    "                continue\n",
    "            try:\n",
    "                href = old_club.find(\"a\", {\"class\":\"tm-player-transfer-history-grid__club-link\"})[\"href\"]\n",
    "            except TypeError as e:\n",
    "                href = old_club.find(\"a\")[\"href\"]\n",
    "            transfer_year = href.split(\"/\")[-1]\n",
    "            club_id = href.split(\"/\")[-3]\n",
    "            hrefs.append(href)\n",
    "            transfer_years.append(transfer_year)\n",
    "            club_ids.append(club_id)\n",
    "    player_info[\"transfer_hrefs\"] = hrefs\n",
    "    player_info[\"transfer_years\"] = transfer_years\n",
    "\n",
    "    # current club id\n",
    "    current_club_id = []\n",
    "    for entry in grid:\n",
    "        old_club = entry.find(\"div\", {\"class\":\"tm-player-transfer-history-grid__new-club\"})\n",
    "        if None == old_club: # handle None matches\n",
    "            continue\n",
    "        else:\n",
    "            if \"grid__heading\" in old_club[\"class\"]:# exclude the header\n",
    "                continue\n",
    "            href = old_club.find(\"a\")[\"href\"]\n",
    "            club_id = href.split(\"/\")[-3]\n",
    "            current_club_id.append(club_id)\n",
    "            break # stop after the first found element\n",
    "    player_info[\"current_club\"] = current_club_id\n",
    "    player_info[\"transfer_club_ids\"] = club_ids\n",
    "\n",
    "    # market value\n",
    "    current_mv = bs.find_all(\"div\", {\"class\":\"tm-player-market-value-development__current-value\"})\n",
    "    for entry in current_mv:\n",
    "        player_current_mv = entry.get_text()\n",
    "        player_info[\"current_mv\"] = player_current_mv\n",
    "\n",
    "    # max market value\n",
    "    max_mv = bs.find_all(\"div\", {\"class\":\"tm-player-market-value-development__max-value\"})\n",
    "    for entry in max_mv:\n",
    "        player_max_mv = entry.get_text()\n",
    "        player_info[\"max_mv\"]= player_max_mv\n",
    "    \n",
    "    # position\n",
    "    player_positions = []\n",
    "    positions = bs.find_all(\"dd\", {\"class\":\"detail-position__position\"})\n",
    "    for position in positions:\n",
    "        player_positions.append(position.get_text())\n",
    "    if len(player_positions)>0:\n",
    "        player_main_position = [player_positions[0]]\n",
    "        player_info[\"main_position\"] = player_main_position\n",
    "    if len(player_positions)>1: \n",
    "        player_other_positions = player_positions[1:]\n",
    "        player_info[\"other_positions\"] = player_other_positions\n",
    "\n",
    "    # nationality\n",
    "    player = bs.find('div', {'class': 'info-table'})\n",
    "    nations = player.find_all(\"img\", {\"class\": \"flaggenrahmen\"})\n",
    "    player_nations = []\n",
    "    for nation in nations:\n",
    "        if \"lazy\" in nation[\"class\"]:\n",
    "            continue\n",
    "        player_nations.append(nation[\"title\"])\n",
    "    player_info[\"nationality\"] = player_nations\n",
    "\n",
    "    player_info = {key: [value] for key, value in player_info.items()}\n",
    "    player_info_df = pd.DataFrame(player_info)\n",
    "    return player_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def load_players_info_for_team(team_url, base_url):\n",
    "    # combine the information derived from get_players and get_player_info\n",
    "    team_id = team_url.split('/')[6]\n",
    "    df = get_players(team_url)\n",
    "    # stop here and then continue with additional info\n",
    "    additional_df = pd.DataFrame()\n",
    "    for href in df[\"player_href\"]:#tqdm(df['player_href'], total=len(df)):#df.player_href:\n",
    "        # print(f\"Starting {list(df.player_href).index(href)+1}/{len(list(df.player_href))} - {datetime.datetime.now()}\")\n",
    "        href = base_url + href\n",
    "        player_info_df = get_player_info(href)\n",
    "        additional_df = pd.concat([additional_df, player_info_df], axis=0)\n",
    "    df = pd.merge(df, additional_df, on=\"player_id\", how=\"left\")\n",
    "    return df\n",
    "\n",
    "def get_players_for_all_teams(df):\n",
    "    # get the players for all teams in the league\n",
    "    players_df = pd.DataFrame()\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # maybe dont include the additional info for players but only scrape the team site\n",
    "        player_df=load_players_info_for_team(\"https://www.transfermarkt.com\" + row.href, \"https://www.transfermarkt.com\")\n",
    "        # player_df = get_players(\"https://www.transfermarkt.com\" + row.href)\n",
    "        players_df = pd.concat([players_df, player_df], axis=0)\n",
    "    if \"Unnamed: 0\" in players_df.columns:\n",
    "        players_df = players_df.drop(\"Unnamed: 0\", axis=1)\n",
    "    '''\n",
    "    columns_with_list_type = [\"transfer_years\", \"transfer_hrefs\", \"transfer_club_ids\", \"main_position\", \"other_positions\", \"nationality\"]\n",
    "    columns_with_list_type = [column for column in columns_with_list_type if column in players_df.columns]\n",
    "    for column in columns_with_list_type:\n",
    "        players_df[column] = players_df[column].apply(lambda x: str(x))\n",
    "    subset = [\"players\", \"player_href\", \"player_id\"]\n",
    "    '''\n",
    "    players_df = players_df.drop_duplicates(subset = \"player_id\").reset_index(drop=True)\n",
    "    return players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir(\"C:/Users/555ka/MMDS/Automatic_Downloader/Web Data Integration/Web-Data-Integration/Webscraping\")\n",
    "print(os.path.exists(\"players_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1/4 - 2023-10-05 08:59:26.081296\n",
      "Finished 1/4 - 2023-10-05 08:59:28.928540\n",
      "Starting 2/4 - 2023-10-05 08:59:28.928540\n",
      "Finished 2/4 - 2023-10-05 08:59:30.792727\n",
      "Starting 3/4 - 2023-10-05 08:59:30.792727\n",
      "Finished 3/4 - 2023-10-05 08:59:32.590849\n",
      "Starting 4/4 - 2023-10-05 08:59:32.590849\n",
      "Finished 4/4 - 2023-10-05 08:59:34.644842\n",
      "Starting run 1 - 2023-10-05 08:59:34.650359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6/23 [07:59<22:39, 79.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 2 - 2023-10-05 09:07:34.374202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/23 [01:40<36:42, 100.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 3 - 2023-10-05 09:09:14.526624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [17:04<00:00, 44.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created File\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Acess the teams for given leagues and years\n",
    "# leagues = [\"Premier League\", 'Jupiler Pro League', \"Bundesliga\",'3. Liga', \"Serie A\", \"La Liga\", \"Ligue 1\", \"Eredivisie\",\n",
    "#             'Championship', 'La Liga 2', 'Serie B', '2. Bundesliga', 'Ligue 2', 'Liga Portugal', 'Super Lig', 'Major League Soccer',\n",
    "#             \"Pro League\", \"Super League\"]\n",
    "\n",
    "# league_urls = {\"/bundesliga/startseite/wettbewerb/L1\":1, \"/premier-league/startseite/wettbewerb/GB1\":2,\n",
    "#                 \"/primera-division/startseite/wettbewerb/ES1\":3, \"/serie-a/startseite/wettbewerb/IT1\":4,\n",
    "#                 \"/ligue-1/startseite/wettbewerb/FR1\":5 }\n",
    "\n",
    "\n",
    "# /liga-portugal/startseite/wettbewerb/PO1\n",
    "# /super-lig/startseite/wettbewerb/TR1\n",
    "# /jupiler-pro-league/startseite/wettbewerb/BE1\n",
    "# /eredivisie/startseite/wettbewerb/NL1\n",
    "\n",
    "# league_urls = {\"/bundesliga/startseite/wettbewerb/L2\":6, \"/premier-league/startseite/wettbewerb/GB2\":7,\n",
    "#                \"/primera-division/startseite/wettbewerb/ES2\":8, \"/serie-a/startseite/wettbewerb/IT2\":9,\n",
    "#                \"/ligue-1/startseite/wettbewerb/FR2\":10}\n",
    "\n",
    "# league_urls = {\"/liga-portugal/startseite/wettbewerb/PO1\": 11, \"/super-lig/startseite/wettbewerb/TR1\": 12,\n",
    "#                \"/jupiler-pro-league/startseite/wettbewerb/BE1\": 13, \"/eredivisie/startseite/wettbewerb/NL1\": 14,}\n",
    "\n",
    "league_urls = {\"/major-league-soccer/startseite/wettbewerb/MLS1\":15, \"/3-liga/startseite/wettbewerb/L3\": 16,\n",
    "               \"/saudi-professional-league/startseite/wettbewerb/SA1\": 17, \"/chinese-super-league/startseite/wettbewerb/CSL\": 18}\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "for league_url in league_urls.keys():\n",
    "    print(f\"Starting {list(league_urls.keys()).index(league_url)+1}/{len(list(league_urls.keys()))} - {datetime.datetime.now()}\")\n",
    "    df = get_teams(\"https://www.transfermarkt.com\" + league_url, [2023], league_urls[league_url])\n",
    "    combined_df = pd.concat([df, combined_df], axis=0)\n",
    "    print(f\"Finished {list(league_urls.keys()).index(league_url)+1}/{len(list(league_urls.keys()))} - {datetime.datetime.now()}\")\n",
    "\n",
    "# print(combined_df[\"href\"])\n",
    "\n",
    "\n",
    "# access the players for given teams\n",
    "df = combined_df.iloc[60:,:].copy()\n",
    "i=0\n",
    "while (not os.path.exists(f\"players_df_others_80.csv\")):\n",
    "    try:\n",
    "        i+=1\n",
    "        print(f\"Starting run {i} - {datetime.datetime.now()}\")\t\n",
    "        players_df = get_players_for_all_teams(df.iloc[:,:])\n",
    "        # print(players_df.head())\n",
    "        players_df[\"current_mv\"] = players_df[\"current_mv\"].apply(lambda x: str(x).replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "        players_df[\"max_mv\"] = players_df[\"max_mv\"].apply(lambda x: str(x).replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "        players_df[\"Birthday\"] = pd.to_datetime(players_df['Birthday'].str.extract('(\\w+ \\d{1,2}, \\d{4})')[0])\n",
    "        players_df.to_csv(f\"players_df_others_80.csv\", index=False)\n",
    "        print(\"Created File\")\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9867\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
